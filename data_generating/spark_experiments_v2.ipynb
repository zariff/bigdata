{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init('/home/max/Downloads/spark-2.4.0-bin-hadoop2.7')\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import Row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark= SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import csv\n",
    "import sys\n",
    "from pympler.asizeof import asizeof\n",
    "import boto3\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import code counts, and convert to dict with probabilities\n",
    "code_counts = pd.read_csv('code_counts.csv')\n",
    "code_counts.columns = ['code', 'count']\n",
    "code_counts['prop'] = code_counts['count']/code_counts['count'].sum()\n",
    "#code_counts.head()\n",
    "code_counts = dict(zip(code_counts['code'], code_counts['prop']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C02'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get random code based on the weight (probability) of each code\n",
    "def get_random_code(dct):\n",
    "    rand_val = random.random()\n",
    "    total = 0\n",
    "    for k, v in dct.items():\n",
    "        total += v\n",
    "        if rand_val <= total:\n",
    "            return k\n",
    "    assert False, 'unreachable'\n",
    "    \n",
    "get_random_code(code_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "filesize = 120000000 # 120mb\n",
    "max_size = 1000000000 # 1gb\n",
    "total_files = max_size/filesize\n",
    "print(int(total_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing 120.0 MB in each file, in a total of 16 files\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-f253b0c4c19c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mmax_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_total_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfilesize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m#filesize=12000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mcreate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilesize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-161-f253b0c4c19c>\u001b[0m in \u001b[0;36mcreate_datasets\u001b[0;34m(filesize, max_files)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mrnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrnd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrnd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_random_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3b5d06542bea>\u001b[0m in \u001b[0;36mget_random_code\u001b[0;34m(dct)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrand_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrand_val\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_value = 5\n",
    "start_time_shift =20\n",
    "random_walk_weight = 0.01\n",
    "random_walk_variance = 1\n",
    "seasonality_1_freq = 50 #lower is shorter\n",
    "seasonality_1_weight = 5\n",
    "seasonality_2_freq = 7 #lower is shorter\n",
    "seasonality_2_weight = 2\n",
    "\n",
    "num_words_in_text = 100 # number of words in each patent text\n",
    "\n",
    "sample_size = 1000\n",
    "\n",
    "trend = 0.02\n",
    "exponent = 1\n",
    "n_samples = 10\n",
    "bucket_name = 'patents-bucket'\n",
    "\n",
    "times = [0,0,0,0,0,0]\n",
    "\n",
    "terms = []\n",
    "with open('wordlist.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    terms = [word[0] for word in list(reader)]\n",
    "    random.shuffle(terms)\n",
    "        \n",
    "# store dataframe to S3 in CSV format\n",
    "def store_data(df, file_num):\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Object(bucket_name, 'patent-100mbs/patents_'+str(file_num)+'.csv').put(Body=csv_buffer.getvalue())\n",
    "\n",
    "# Get number of samples to be made for one day\n",
    "def get_num_samples(base):\n",
    "    #Assume no start at y = 0\n",
    "    random_walk = np.random.normal(0,random_walk_variance)\n",
    "    time = base + start_time_shift\n",
    "\n",
    "    y = (np.sin(time/seasonality_1_freq))*seasonality_1_weight #Add seasonality 1\n",
    "    y = y + np.sin(time/seasonality_2_freq)*seasonality_2_weight #Add seasonality 2\n",
    "    y = y + (trend * (time ** exponent)) #Add trend\n",
    "\n",
    "    # Random walk component\n",
    "    random_walk += np.random.normal(0,random_walk_variance)\n",
    "    \n",
    "    return max(0, y + random_walk * random_walk_weight)\n",
    "\n",
    "\n",
    "def create_datasets(filesize, max_files):\n",
    "    print(\"Storing\", filesize/1000000, \"MB in each file, in a total of\", max_files, \"files\")\n",
    "    rows = []\n",
    "    sample_range = 0\n",
    "    file_num = 0\n",
    "    \n",
    "    i = 0\n",
    "    while file_num < max_files:\n",
    "\n",
    "        if sys.getsizeof(rows) > filesize: # larger than filesize (120mb?), store file\n",
    "            df = pd.DataFrame(rows, columns=['date', 'code', 'text'])\n",
    "            #print(\"stored\", sys.getsizeof(rows)/1000000, \"MB in file num\", file_num)\n",
    "            store_data(df, file_num)\n",
    "            print(\"stored\", sys.getsizeof(rows)/1000000, \"MB in file num\", file_num)\n",
    "            rows = []\n",
    "            file_num+=1\n",
    "            \n",
    "        num_samples = get_num_samples(i)\n",
    "        \n",
    "        for n in range(int(round(num_samples))): #Create n entries\n",
    "            date = datetime.datetime(1950, 1, 1, 0, 0) + timedelta(days=int(i))\n",
    "            current_sample = i+n\n",
    "            #sample = terms[current_sample:current_sample+100]\n",
    "            #text = \" \".join(random.sample(terms, num_words_in_text)) # TOO SLOW\n",
    "            rnd = random.randint(1,len(terms)-101)\n",
    "            text = \" \".join(terms[rnd:rnd+100]) \n",
    "            rows.append([date, get_random_code(code_counts), text])\n",
    "        \n",
    "        \n",
    "        \n",
    "        i+=1\n",
    "\n",
    "filesize = 120000000 # 120mb\n",
    "max_total_size = 2000000000 # 5gb\n",
    "#filesize = 1200 #\n",
    "#max_total_size = 4000 #\n",
    "max_files = int(max_total_size/filesize)\n",
    "#filesize=12000\n",
    "create_datasets(filesize, max_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
